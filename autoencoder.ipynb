{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPsehktHXdHGPWrdG0eaGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iambikash378/ML-Stuff/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "teR8a6yzFgy4",
        "outputId": "601fd10f-b7d7-4dfe-e870-23502ac0668c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vector_quantize_pytorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e65f6e5cd52d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvector_quantize_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorQuantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vector_quantize_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from vector_quantize_pytorch import VectorQuantize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and initialzing the MNIST Trainin and Testing dataset\n",
        "\n",
        "transform = tv.transforms.Compose([tv.transforms.ToTensor(), tv.transforms.Normalize((0.1307,),(0.3081,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train= True, download = False, transform = transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train = False, download = True, transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size = 5, shuffle = False)\n"
      ],
      "metadata": {
        "id": "PITAtoo_FkMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Autoencoder,self).__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Conv2d(1,4,kernel_size = 5),\n",
        "      nn.ReLU(True),\n",
        "      nn.Conv2d(4, 8, kernel_size =5),\n",
        "      nn.ReLU(True),\n",
        "    )\n",
        "\n",
        "    self.quantizer = VectorQuantize(\n",
        "       dim = 20,\n",
        "       codebook_size = 8,\n",
        "       decay = 0.8,\n",
        "       commitment_weight = 1\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Linear(10, 400),\n",
        "      nn.ReLU(True),\n",
        "      nn.Linear(400, 4000),\n",
        "      nn.ReLU(True),\n",
        "      nn.Unflatten(1, (10, 20 , 20)),\n",
        "      nn.ConvTranspose2d(10, 10, kernel_size = 5),\n",
        "      nn.ConvTranspose2d(10,1, kernel_size =5),\n",
        "      nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    enc = self.encoder(x)\n",
        "    qtz, _ , _ = self.quantizer(enc)\n",
        "    dec = self.decoder(qtz)\n",
        "    return dec\n"
      ],
      "metadata": {
        "id": "mSnTzcZwFohv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Hyperparameters\n",
        "\n",
        "num_epochs = 1\n",
        "batch_size = 128\n",
        "model = Autoencoder().cpu()\n",
        "distance = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum = 0.5)\n"
      ],
      "metadata": {
        "id": "v0OpJAhrFtPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "  for data in train_loader:\n",
        "    img, _ = data\n",
        "    img = Variable(img).cpu()\n",
        "    output = model(img)\n",
        "    loss = distance(output, img)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print('epoch [{}/{}], loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "torch.save(model.state_dict(),'trained_model.pth')"
      ],
      "metadata": {
        "id": "x_NdjMJyFxoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Get some test images\n",
        "images, _ = next(iter(test_loader))\n",
        "\n",
        "# Reconstruct images\n",
        "with torch.no_grad():\n",
        "    reconstructed_images = model(images)\n",
        "\n",
        "# Display original and reconstructed images\n",
        "num_images = 5\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(num_images):\n",
        "    # Original image\n",
        "    plt.subplot(2, num_images, i + 1)\n",
        "    plt.imshow(np.transpose(images[i], (1, 2, 0)).squeeze(), cmap='gray')\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstructed image\n",
        "    plt.subplot(2, num_images, i + 1 + num_images)\n",
        "    plt.imshow(np.transpose(reconstructed_images[i], (1, 2, 0)).squeeze(), cmap='gray')\n",
        "    plt.title('Reconstructed')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X8lBfJzqF12E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}